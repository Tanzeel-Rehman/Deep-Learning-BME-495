**Home Work \#4**

**<u>PART B API (NnImg2Num):</u>**

The MNIST handwritten dataset was trained using the pytorch ‘nn’ package
using a batch size of 60. The network architecture used for training was
four layered (28\*28, 100, 50, 30, 10). The sigmoid was used as an
activation function for forward pass. Pytorch optim package was used to
update the parameters using SGD with a learning rate of 0.001 and
momentum term of 0.9. The mean square error was used as a loss function.

<img src="media\image1.png" style="width:5.55208in;height:4.16667in" />

**Figure 1: Training loss (Y-axis) Vs the epochs (X-axis)**

<img src="media\image2.png" style="width:6.5in;height:3.65625in" alt="Graphical user interface, text, application Description automatically generated" />
